# Artifact Description
## Data Race Detection by Digest-Driven Abstract Intepretation

This is the artifact description for our manuscript **Data Race Detection by Digest-Driven Abstract Intepretation**.

The artifact contains [Goblint at `race_digest_staging` branch](https://github.com/goblint/analyzer/tree/race_digest_staging).


## Overview

This artifact contains the following components:

    Evaluation Results  ::  The evaluation results, and overview tables (HTML)
                            generated from the raw data.
    Source Code         ::  Source code for Goblint.
    Verifier Binary     ::  Binary for Goblint.
    Benchmark Programs  ::  Benchmarks used for evaluation (SV-COMP 2025).
    BenchExec Tool      ::  The BenchExec benchmarking tool can be used
                            to replicate our results on these benchmarks.

The next section gives instructions on how to setup and quickly get an overview of the artifact.
The subsequent sections then describe each of these components in detail.
The final section gives information on how to reuse this artifact.


## Getting Started

### 1. Setup

The artifact is a virtual machine (VM), as the Benchexec system used for reliable benchmarking cannot be installed in docker images. The machine is intended for an x86 machine, behavior on M1 chips is untested.
Follow these steps to set it up:

* If you have not done so already, install VirtualBox.
  (<https://www.virtualbox.org/wiki/Downloads>)
* Download the artifact.
* Import the artifact via the VirtualBox UI (`File > Import Appliance`).

You can then start the VM from the VirtualBox UI.
Login with user `goblint` and password `goblint`.

### 2. Inspect the evaluation results



### 3. Quick Test of the Benchmark Setup

To check Goblint is properly installed, run 

    cd ~/analyzer
    make sanitytest

on a console in the VM.
The run should take no longer than 2 minutes, the expected output is

    Excellent: ignored check on tests/regression/03-practical/21-pfscan_combine_minimal.c:21 is now passing!
    Excellent: ignored check on tests/regression/03-practical/21-pfscan_combine_minimal.c:29 is now passing!
    No errors :)



### 4. Running the Full Experiments

To re-run the full experiments, execute

    ~/scripts/full-run.sh

This script behaves similarly to the smaller variants in the previous sections.
Note however:

- By default the experiments require 16 GB of memory per benchmark (this configuration was used for the experiments in the paper).
    To reproduce this, you will have to modify the VM's settings in VirtualBox and increase the available memory (shutdown the machine while doing so).

    Alternatively, you can run the experiments with a reduced memory limit.
    To do so, modify the environment variable `BENCHMARK_PARAMS`.
    For instance, the following allows only 4GB of memory per benchmark:

        BENCHMARK_PARAMS="-M 4GB" ~/scripts/full-run.sh

- The full evaluation for the paper required around 3 days.
    In this evaluation, we used the benchexec tool to run 14 validation tasks in parallel (occupying up to 28 cores at a time).

    By default, the provided script only runs one benchmark at a time.
    If you have sufficient cores and memory available (adjust the VM settings accordingly), you can run multiple benchmarks in parallel by setting the environment variable `BENCHEXEC_THREADS`. You may also execute the experiments with a reduced timeout.
    For instance, the following command runs 4 benchmarks in parallel at a time (occupying up to 8 cores), and gives each benchmark a 300s timeout and 4GB memory limit:

        BENCHEXEC_THREADS=4 BENCHMARK_PARAMS="-T 300s -M 4GB" ~/scripts/full-run.sh

Naturally, changes to the timeout or memory are expected to affect the evaluation numbers.


## Evaluation Results

The evaluation results that form the basis for the experimental data in the paper can be found in the directory `~/witness-validation/paper-evaluation/`.
The witnesses generated by Goblint that formed the basis for this evaluation can be found in `~/witness-generation/paper-evaluation/`.
See below for detailed info to reproduce the tables and figures of the paper.

The file `~/witness-validation/paper-evaluation/` contains an HTML overview page generated by the BenchExec benchmarking tool, which displays individual results, quantile and scatter plots. Through the filtering sidebar (top right corner), detailed analyses can be made.

The table contains the following configurations :

* `verify` -- GemCutter verification, without any witness
* `verify+validate-goblint-witnesses-{mutex-meet,protection}-{ghost,local}` -- GemCutter witness validation, applied to the 4 different witness sets generated by Goblint.
  In this mode, GemCutter checks if the given witness is valid and the corresponding program is correct.
* `validate-goblint-witnesses-{mutex-meet,protection}-{ghost,local}` -- GemCutter witness _confirmation_, applied to the different witness sets.
  In this mode, described in the paper, GemCutter only checks if the given witness' invariants are correct, but does not prove the corresponding program correct.

The summary table shows how many benchmarks were analysed and the results.

- The row `correct true` indicates tasks that were successfully verified (for `verify`), or where the witness was confirmed resp. validated (for the other configurations).
- The row `correct false` indicates that a bug was found (for `verify`), resp. that witness validation failed and a witness was rejected.
  The latter only happens for programs that are incorrect, hence there can be no valid correctness witness and rejection is expected.
  As witness validation is not possible for incorrect programs, this data is not discussed in the paper and only appears here due to the benchmark setup.
- The row `incorrect false` would indicate that GemCutter finds a supposed bug in a correct program (for `verify`).
  For witness confirmation configurations, it indicates that GemCutter confirmed the correctness witness given by Goblint, for an incorrect program.
  As witness confirmation ignores the program's correctness, these results are expected and do not indicate a problem in one of the tools.

The *Table* tab gives access to detailed evaluation results for each file.
Clicking on a status shows the complete GemCutter log for the benchmark run.

> **Note:** If you are trying to view logs for individual runs through the HTML table (by clicking on the evaluation result `true` or `false`), you may encounter a warning because browsers block access to local files. Follow the instructions in the message to enable log viewing.

As described above (in [section _2. Inspect the evaluation results_](#2-inspect-the-evaluation-results)), the artifact provides python scripts to directly extract the data shown in the paper from the benchmark results.


## Source Code


### Goblint

The Goblint analyzer (<https://goblint.in.tum.de>) is developed by TU Munich and University of Tartu. The source code for Goblint at the time of evaluation can be found in this artifact in the `~/analyzer` directory.

The code for this paper is the following:

**TODO**

More recent versions of Goblint can be found at <https://github.com/goblint>.


## Verifier Binaries

A pre-built binary of Goblint is available as `~/analzyer/goblint`.
See `~/analyzer/GOBLINT_README.md` for information on how to run Goblint.
To build Goblint from scratch, run `make release`.


## Benchmark Programs

This artifact includes the benchmark programs on which we evaluated the verifiers.
These benchmarks are taken from the publicly available sv-benchmarks set (<https://gitlab.com/sosy-lab/benchmarking/sv-benchmarks>)
and correspond to the _ConcurrencySafety-Main_ category of SV-COMP'24 (<https://sv-comp.sosy-lab.org/2024/>).
The benchmarks are written in C and use POSIX threads (`pthreads`) to model concurrency.


## Extending & Reusing This Artifact

* **Adding benchmarks:** You can easily add your own benchmarks programs written in C.
    C programs should contain an empty function called `reach_error()`. Goblint and GemCutter then check that this function is never invoked. Certain (gcc) preprocessing steps may be necessary, e.g. to resolve `#include`s. See the SV-COMP benchmarks for examples (the preprocessed files typically have the extension `.i`).

    To run the evaluation on your own programs, you must edit the benchmark definition files `~/witness-generation/goblint.xml.template` resp. `~/witness-validation/gemcutter.xml.template`.
    Replace the `<include>` path specified in the task set `minimal` with your own path.
    You can then simply run `~/scripts/quick-run.sh`.

## Acknowledgment

This description is based, in part, on earlier artifact descriptions for the Goblint system, in particular of our VMCAI '24 & '25 and ESOP '23 papers.